# Multi-View Driver Head Action Analysis

**Person Multi-View Sensing for Driver Action Understanding under Visual Field Impairment**

This repository provides a **third-person, multi-view video framework** that analyzes driver head/actions from **three external cameras (left/front/right)**.
Unlike typical in-cabin or driver-centric perception, we study **humanâ€“system cooperative perception**: the **driver is perceptually constrained**, while the **system has a richer multi-view observation**.

Our central goal is not merely higher classification accuracy, but to understand:

> **How can a multi-view sensing system infer driver actions robustly and provide compensation signals when the driverâ€™s perceptual access is limited?**

This framing targets **SIGCHI** audiences interested in **assistive interfaces, cooperative perception, human-centered sensing, and reliability under human constraints**.

---

## Why SIGCHI? Human-Centered Motivation

Drivers with **visual field impairments** may struggle to perceive (or confirm) their own head movements and surrounding cues.
However, an external sensing system can observe the same behavior from multiple viewpoints and potentially provide:

- **Awareness support** (e.g., â€œyou checked left mirrorâ€ confirmation)
- **Safety feedback** (e.g., missing checks / delayed checks)
- **Robust monitoring** under partial sensor failure

**Key premise:** the limitation lies in the humanâ€™s perceptionâ€”not in camera sensing.

---

## Contributions

This project contributes:

1. **A third-person multi-view driver action analysis pipeline** for studying humanâ€“system cooperative perception under visual constraints.
2. **View contribution and complementarity analysis**, quantifying how each viewpoint helps (or fails) across actions.
3. **Robustness evaluation under view dropout**, approximating real-world sensing degradation.
4. _(Optional)_ A modular baseline suite that enables **fair comparison** of fusion strategies with a shared backbone.

---

## System Overview

**Left / Front / Right videos** are synchronized and processed with a **shared backbone** (weight-tied across views) for fairness.

```
Left  â”
Front â”œâ”€â”€â–º Shared Backbone (per-view) â”€â–º Feature Fusion â”€â–º Action Prediction
Right â”˜
```

Fusion is performed at the **feature level**:

- Average fusion
- Concatenation + MLP
- Optional view-weighting (attention / reliability scoring)

---

## Key Features

### Multi-View Inputs

- ğŸ¥ Synchronized **Left / Front / Right** third-person cameras

### Driver Action Understanding

- ğŸ§  8 predefined action classes (extensible)
- Frame-level or clip-level labeling supported

### Fusion & Analysis

- ğŸ”€ Single-view baselines
- ğŸ”€ Multi-view fusion strategies (avg / concat / weighting)
- ğŸ“Š View contribution analysis: **Single-view**, **LOVO**, **pairwise complementarity**
- ğŸ§ª Robustness tests: **view drop at inference**

---

## å½“å‰å¯åšçš„å¯¹æ¯”å®éªŒï¼ˆåŸºäº config é€‰é¡¹ï¼‰

### 1) å•è§†è§’è¾“å…¥ï¼ˆtrain.view=singleï¼‰
- **RGB å•è§†è§’**ï¼š`model.input_type=rgb` + `model.backbone=3dcnn|transformer|mamba`

### 2) ä¸‰è§†è§’è¾“å…¥ï¼ˆtrain.view=multiï¼‰
- **RGB ä¸‰è§†è§’**ï¼š`model.input_type=rgb` + `model.backbone=3dcnn|transformer|mamba` + `model.fuse_method=late`

### 3) å¤šè§†è§’èåˆæ–¹å¼ï¼ˆlate fusionï¼‰
- **logit/prob èåˆ**ï¼š`model.fusion_mode=logit_mean|prob_mean`
- **ç‰¹å¾çº§èåˆ**ï¼š`model.fusion_mode=feature_mean|feature_concat`

### 4) å¤šè§†è§’èåˆæ–¹å¼ï¼ˆearly fusionï¼‰
- **åŠ æƒ/æ‹¼æ¥èåˆ**ï¼š`model.fuse_method=add|mul|concat|avg`

### 5) å¤šè§†è§’èåˆæ–¹å¼ï¼ˆmid fusionï¼‰
- **SE æ³¨æ„åŠ›èåˆ**ï¼š`model.fuse_method=se_attn`ï¼ˆæ—§é…ç½®å¯ç”¨ `se_atn`ï¼‰

---

## Dataset Structure

```
data/
â”œâ”€â”€ subject_01/
â”‚   â”œâ”€â”€ left/video.mp4
â”‚   â”œâ”€â”€ front/video.mp4
â”‚   â”œâ”€â”€ right/video.mp4
â”‚   â””â”€â”€ labels.csv
```

- Views are **temporally synchronized**
- Experiments should be split **by subject/session** to avoid leakage

---

## Driver Action Classes (Example)

1. Looking forward
2. Checking left mirror
3. Checking right mirror
4. Operating dashboard
5. Steering adjustment
6. Lane checking
7. Idle driving
8. Other driver actions

---

## Evaluation

### Metrics (for performance + human-centered reliability)

- Accuracy
- Macro F1
- Per-class recall
- Confusion matrix

### View Contribution

- **Single-view**: L / F / R
- **Leave-One-View-Out (LOVO)**: quantify marginal utility
- **Pairwise fusion**: (L+F), (F+R), (L+R)

### Robustness (system reliability)

- Random view removal at inference
- Performance degradation curves under partial sensing

These analyses measure:

- **Independent view quality**
- **Marginal contribution**
- **Complementarity**
- **Graceful degradation**

---

## Reproducibility

- Deterministic seed setting
- Subject/session split config
- Modular fusion components
- Logging of confusion matrices and per-class recall

> SIGCHI ä¼šå¾ˆçœ‹é‡ï¼šä½ èƒ½ä¸èƒ½æŠŠå®éªŒè·‘å¾—å‡ºæ¥ã€èƒ½ä¸èƒ½è§£é‡Šç³»ç»Ÿä»€ä¹ˆæ—¶å€™ä¼šå¤±è´¥ã€‚

---

## Ethics & Intended Use

This repository is intended for **research on assistive sensing** and **humanâ€“system cooperative perception**, not for surveillance or punitive monitoring.
If you plan to deploy in real settings, consider:

- consent + transparency
- data minimization
- privacy protection
- bias across impairment types and driving contexts

---

## Citation

```bibtex
@inproceedings{Chen202XCHI,
  title     = {Third-Person Multi-View Driver Action Analysis for Cooperative Perception under Visual Field Impairment},
  author    = {Chen, Kaixu},
  booktitle = {CHI Conference on Human Factors in Computing Systems},
  year      = {202X}
}
```
