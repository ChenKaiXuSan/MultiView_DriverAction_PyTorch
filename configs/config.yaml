# hydra config
hydra:
  run:
    dir: ${log_path} # the log path, used for log and checkpoint
  sweep:
    dir: logs/
    subdir: ${experiment}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweeper:
    params:
      model.fuse_method: pose_atn
      model.fusion_layers: 0, 1, 2, 3, 4, 5
      model.backbone: 3dcnn

loss:
  lr: 0.0001
  beta1: 0.5 # decapture
  beta2: 0.999 # decapture

  selection: ["cls", "attn_loss", "bg", "tmp"] # the loss selection for the total loss

  lambda_list: [0.25, 0.5, 0.75, 1.0] # the lambda list for the total loss
  w_bg: 0.2 # the weight of background class for the loss function, used for cross validation with different class number
  w_temp: 0.05 # the weight of temporal smoothness for the loss function, used for cross validation with different class number

paths:
  root_path: /workspace/data/multi_view_driver_action

  annotation_path: ${paths.root_path}/label
  index_mapping: ${paths.root_path}/index_mapping # training mapping path, this used for cross validation, with different class number.
  start_mid_end_path: ${paths.root_path}/split_mid_end/mini.json # æˆ‘ä»¬ä¸éœ€è¦æ¨å¯¼å·¦å³çš„frameï¼ŒæŒ‰ç…§è¿™é‡Œé¢çš„startå’Œmidæ¥è¿›è¡Œæ¨å¯¼

  video_path: /workspace/data/videos_split
  sam3d_results_path: /workspace/data/sam3d_body_results_right

data:
  num_workers: 12
  img_size: 224

  uniform_temporal_subsample_num: 8 # num frame from the clip duration, f or define one gait cycle, we need use whole frames.

  batch_size: 1 # load video number

  fold: 5 # the fold number of the cross validation

  # æ§åˆ¶æ•°æ®åŠ è½½
  load_rgb: true   # åŠ è½½è§†é¢‘å¸§
  load_kpt: false  # ä¸åŠ è½½å…³é”®ç‚¹ï¼ˆèŠ‚çœå†…å­˜å’Œæ—¶é—´ï¼‰
  
  # ğŸ”‘ å…³é”®å‚æ•°ï¼šåˆ†å—åŠ è½½
  # å¦‚æœvideoè¶…è¿‡è¿™ä¸ªå¸§æ•°ï¼Œä¼šè¢«è‡ªåŠ¨åˆ†æˆå¤šä¸ªchunks
  # æ¯æ¬¡åªåŠ è½½ä¸€ä¸ªchunkï¼Œé¿å…åŠ è½½OOM
  max_video_frames: 500 # æ¨èå€¼ï¼š500-2000
  
  # max_video_frames é€‰æ‹©æŒ‡å—ï¼š
  # - 224Ã—224 åˆ†è¾¨ç‡ï¼š500-1000
  # - 112Ã—112 åˆ†è¾¨ç‡ï¼š1000-2000  
  # - æ›´å¤§åˆ†è¾¨ç‡ï¼š300-500
  # åŸåˆ™ï¼šå°½é‡å¤§ï¼ˆå‡å°‘chunksæ•°é‡ï¼‰ï¼Œä½†ä¸è¦OOM
  
  # ğŸš€ ä¼˜åŒ–å‚æ•°ï¼šå¹¶è¡ŒåŠ è½½
  # æ§åˆ¶å¤šè§†è§’å’Œå…³é”®ç‚¹å¹¶è¡ŒåŠ è½½çš„çº¿ç¨‹æ•°
  # å¢åŠ æ­¤å€¼å¯æå‡I/Oæ€§èƒ½ï¼ˆéœ€è¦å¿«é€Ÿå­˜å‚¨æ”¯æŒï¼‰
  num_io_threads: 3 # æ¨èå€¼ï¼š3-8
  
  # num_io_threads é€‰æ‹©æŒ‡å—ï¼š
  # - NVMe SSDï¼š6-8ï¼ˆé«˜é€Ÿå­˜å‚¨ï¼‰
  # - SATA SSDï¼š4-6ï¼ˆä¸­é€Ÿå­˜å‚¨ï¼‰
  # - HDDï¼š2-3ï¼ˆæ…¢é€Ÿå­˜å‚¨ï¼‰
  # åŸåˆ™ï¼šå¿«é€Ÿå­˜å‚¨å¯ä½¿ç”¨æ›´å¤šçº¿ç¨‹ï¼Œæ…¢é€Ÿå­˜å‚¨å‡å°‘çº¿ç¨‹é¿å…ç£ç›˜æŠ–åŠ¨

model:
  backbone: 3dcnn # choices=[3dcnn, transformer, mamba, stgcn], help='the backbone of the model'
  model_class_num: 9 # the class num, left, right, up, down, left_up, left_down, right_up, right_down, front
  use_side_heads: True # whether to use side heads for intermediate supervision
  input_type: rgb # rgb | kpt | rgb_kpt
  fusion_mode: logit_mean # logit_mean | prob_mean | feature_mean | feature_concat
  modality_fusion: concat # concat | mean (for rgb+kpt)
  fusion_feature_dim: 256
  stgcn_hidden_dim: 64
  stgcn_layers: 3
  stgcn_temporal_kernel: 3
  stgcn_dropout: 0.1
  stgcn_num_kpts: null
  transformer_dim: 256
  transformer_layers: 4
  transformer_heads: 4
  transformer_ff_dim: 1024
  transformer_dropout: 0.1
  mamba_dim: 256
  mamba_layers: 2
  mamba_dropout: 0.1

  fuse_method: late # add, mul, concat, none, late, avg, cross_atn, se_atn, pose_atn

train:
  # Training config
  max_epochs: 50 # numer of epochs of training

  view: single # single or multi
  view_name: front # used when train.view=single
  feature_map_batches: 10 # dump CAM features for first N test batches

  gpu: 0 # choices=[0, 1], help='the gpu number whicht to train'

log_path: logs/train/${experiment}/${now:%Y-%m-%d}/${now:%H-%M-%S}
experiment: view_${train.view}_${train.view_name}_${model.backbone}_fuse_method_${model.fuse_method} # the experiment name, used for log path
