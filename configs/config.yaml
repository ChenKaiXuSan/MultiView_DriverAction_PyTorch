# hydra config
hydra:
  run:
    dir: ${log_path} # the log path, used for log and checkpoint
  sweep:
    dir: logs/
    subdir: ${experiment}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweeper:
    params:
      model.fuse_method: pose_atn
      model.fusion_layers: 0, 1, 2, 3, 4, 5
      model.backbone: 3dcnn

loss:
  lr: 0.0001
  beta1: 0.5 # decapture
  beta2: 0.999 # decapture

  selection: ["cls", "attn_loss", "bg", "tmp"] # the loss selection for the total loss

  lambda_list: [0.25, 0.5, 0.75, 1.0] # the lambda list for the total loss
  w_bg: 0.2 # the weight of background class for the loss function, used for cross validation with different class number
  w_temp: 0.05 # the weight of temporal smoothness for the loss function, used for cross validation with different class number

paths:
  root_path: /workspace/data/multi_view_driver_action

  annotation_path: ${paths.root_path}/label
  index_mapping: ${paths.root_path}/index_mapping # training mapping path, this used for cross validation, with different class number.
  start_mid_end_path: ${paths.root_path}/split_mid_end/mini.json # 我们不需要推导左右的frame，按照这里面的start和mid来进行推导

  video_path: /workspace/data/videos_split
  sam3d_results_path: /workspace/data/sam3d_body_results_right

data:
  num_workers: 12
  img_size: 224

  batch_size: 1 # load video number
  video_batch_size: 32

  fold: 5 # the fold number of the cross validation

model:
  backbone: 3dcnn # choices=[3dcnn, transformer, mamba, stgcn], help='the backbone of the model'
  model_class_num: 9 # the class num, left, right, up, down, left_up, left_down, right_up, right_down, front
  use_side_heads: True # whether to use side heads for intermediate supervision
  input_type: rgb # rgb | kpt | rgb_kpt
  fusion_mode: logit_mean # logit_mean | prob_mean | feature_mean | feature_concat
  modality_fusion: concat # concat | mean (for rgb+kpt)
  fusion_feature_dim: 256
  stgcn_hidden_dim: 64
  stgcn_layers: 3
  stgcn_temporal_kernel: 3
  stgcn_dropout: 0.1
  stgcn_num_kpts: null
  transformer_dim: 256
  transformer_layers: 4
  transformer_heads: 4
  transformer_ff_dim: 1024
  transformer_dropout: 0.1
  mamba_dim: 256
  mamba_layers: 2
  mamba_dropout: 0.1

  fuse_method: late # add, mul, concat, none, late, avg, cross_atn, se_atn, pose_atn

train:
  # Training config
  max_epochs: 50 # numer of epochs of training

  # used for val
  clip_duration: 1 # clip duration for the video
  uniform_temporal_subsample_num: 8 # num frame from the clip duration, f or define one gait cycle, we need use whole frames.

  view: multi # single or multi
  view_name: front # used when train.view=single
  feature_map_batches: 10 # dump CAM features for first N test batches

  gpu: 0 # choices=[0, 1], help='the gpu number whicht to train'

log_path: logs/train/${experiment}/${now:%Y-%m-%d}/${now:%H-%M-%S}
experiment: view_${train.view}_${train.view_name}_${model.backbone}_fuse_method_${model.fuse_method} # the experiment name, used for log path
