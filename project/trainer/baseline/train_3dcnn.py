#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
File: /workspace/code/project/trainer/train_3dcnn.py
Project: /workspace/code/project/trainer
Created Date: Monday November 10th 2025
Author: Kaixu Chen
-----
Comment:

Have a good code time :)
-----
Last Modified: Monday November 10th 2025 7:22:14 pm
Modified By: the developer formerly known as Kaixu Chen at <chenkaixusan@gmail.com>
-----
Copyright (c) 2025 The University of Tsukuba
-----
HISTORY:
Date      	By	Comments
----------	---	---------------------------------------------------------
"""

from typing import Any, Dict, List, Optional, Union
import logging

import torch
import torch.nn.functional as F

from pytorch_lightning import LightningModule

from torchmetrics.classification import (
    MulticlassAccuracy,
    MulticlassPrecision,
    MulticlassRecall,
    MulticlassF1Score,
    MulticlassConfusionMatrix,
)


from project.models.make_model import select_model

from project.utils.helper import save_helper
from project.utils.save_CAM import dump_all_feature_maps

logger = logging.getLogger(__name__)


class Res3DCNNTrainer(LightningModule):
    def __init__(self, hparams):
        super().__init__()

        self.img_size = hparams.data.img_size
        self.lr = getattr(hparams.loss, "lr", 1e-3)  # default lr

        self.num_classes = hparams.model.model_class_num

        # define model
        self.model = select_model(hparams)
        self.input_type = getattr(hparams.model, "input_type", "rgb")
        self.view_name = getattr(hparams.train, "view_name", "front")
        self.feature_map_dump_batch_limit = int(
            getattr(hparams.train, "feature_map_batches", 10)
        )

        # save the hyperparameters to the file and ckpt
        self.save_hyperparameters()

        self._accuracy = MulticlassAccuracy(num_classes=self.num_classes)
        self._precision = MulticlassPrecision(num_classes=self.num_classes)
        self._recall = MulticlassRecall(num_classes=self.num_classes)
        self._f1_score = MulticlassF1Score(num_classes=self.num_classes)
        self._confusion_matrix = MulticlassConfusionMatrix(num_classes=self.num_classes)

        self.save_root = getattr(hparams.train, "log_path", "./logs")

    def forward(self, x, kpts: Optional[torch.Tensor] = None):
        if self.input_type == "rgb":
            return self.model(x)
        if self.input_type == "kpt":
            return self.model(kpts)
        if self.input_type == "rgb_kpt":
            return self.model(x, kpts)
        raise ValueError(f"Unknown input_type: {self.input_type}")

    def _select_view(self, data: Optional[Union[Dict[str, torch.Tensor], torch.Tensor]]):
        if data is None:
            return None
        if isinstance(data, dict):
            if self.view_name not in data:
                raise KeyError(f"View '{self.view_name}' not found in batch")
            return data[self.view_name]
        return data

    def _prepare_inputs(self, batch: Dict[str, torch.Tensor]):
        video = self._select_view(batch.get("video"))
        kpts = self._select_view(batch.get("sam3d_kpt"))

        if video is not None:
            video = video.detach()
        if kpts is not None:
            kpts = kpts.detach()

        if self.input_type == "kpt" and kpts is None:
            raise ValueError("Keypoint input requested but sam3d_kpt is missing.")
        if self.input_type == "rgb_kpt" and (video is None or kpts is None):
            raise ValueError("RGB+KPT input requested but video or sam3d_kpt is missing.")

        return video, kpts

    @staticmethod
    def _prepare_label(batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        return batch["label"].detach().view(-1)

    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):
        # prepare the input and label
        video, kpts = self._prepare_inputs(batch)
        label = self._prepare_label(batch)

        b = label.shape[0]

        video_preds = self(video, kpts)
        video_preds_softmax = torch.softmax(video_preds, dim=1)

        assert label.shape[0] == video_preds.shape[0]

        loss = F.cross_entropy(video_preds, label.long())

        self.log("train/loss", loss, on_epoch=True, on_step=True, batch_size=b)

        # log metrics
        video_acc = self._accuracy(video_preds_softmax, label)
        video_precision = self._precision(video_preds_softmax, label)
        video_recall = self._recall(video_preds_softmax, label)
        video_f1_score = self._f1_score(video_preds_softmax, label)
        video_confusion_matrix = self._confusion_matrix(video_preds_softmax, label)

        self.log_dict(
            {
                "train/video_acc": video_acc,
                "train/video_precision": video_precision,
                "train/video_recall": video_recall,
                "train/video_f1_score": video_f1_score,
            },
            on_epoch=True,
            on_step=True,
            batch_size=b,
        )
        logger.info(f"train loss: {loss.item()}")

        return loss

    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):
        # input and model define
        video, kpts = self._prepare_inputs(batch)
        label = self._prepare_label(batch)

        b = label.shape[0]

        video_preds = self(video, kpts)
        video_preds_softmax = torch.softmax(video_preds, dim=1)

        loss = F.cross_entropy(video_preds, label.long())

        self.log("val/loss", loss, on_epoch=True, on_step=True, batch_size=b)

        # log metrics
        video_acc = self._accuracy(video_preds_softmax, label)
        video_precision = self._precision(video_preds_softmax, label)
        video_recall = self._recall(video_preds_softmax, label)
        video_f1_score = self._f1_score(video_preds_softmax, label)
        video_confusion_matrix = self._confusion_matrix(video_preds_softmax, label)

        self.log_dict(
            {
                "val/video_acc": video_acc,
                "val/video_precision": video_precision,
                "val/video_recall": video_recall,
                "val/video_f1_score": video_f1_score,
            },
            on_epoch=True,
            on_step=True,
            batch_size=b,
        )

        logger.info(f"val loss: {loss.item()}")

    ##############
    # test step
    ##############
    # the order of the hook function is:
    # on_test_start -> test_step -> on_test_batch_end -> on_test_epoch_end -> on_test_end

    def on_test_start(self) -> None:
        """hook function for test start"""

        self.test_pred_list: list[torch.Tensor] = []
        self.test_label_list: list[torch.Tensor] = []

        logger.info("test start")

    def on_test_end(self) -> None:
        """hook function for test end"""
        logger.info("test end")

    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):
        # input and model define
        video, kpts = self._prepare_inputs(batch)
        label = self._prepare_label(batch)

        b = label.shape[0]

        video_preds = self(video, kpts)
        video_preds_softmax = torch.softmax(video_preds, dim=1)

        loss = F.cross_entropy(video_preds, label.long())

        self.log("test/loss", loss, on_epoch=True, on_step=True, batch_size=b)

        # log metrics
        video_acc = self._accuracy(video_preds_softmax, label)
        video_precision = self._precision(video_preds_softmax, label)
        video_recall = self._recall(video_preds_softmax, label)
        video_f1_score = self._f1_score(video_preds_softmax, label)
        video_confusion_matrix = self._confusion_matrix(video_preds_softmax, label)

        metric_dict = {
            "test/video_acc": video_acc,
            "test/video_precision": video_precision,
            "test/video_recall": video_recall,
            "test/video_f1_score": video_f1_score,
        }
        self.log_dict(metric_dict, on_epoch=True, on_step=True, batch_size=b)

        self.test_pred_list.append(video_preds_softmax.detach().cpu())
        self.test_label_list.append(label.detach().cpu())

        fold = (
            getattr(self.logger, "root_dir", "fold").split("/")[-1]
            if self.logger
            else "fold"
        )
        if (
            self.input_type == "rgb"
            and batch_idx < self.feature_map_dump_batch_limit
            and video is not None
            and batch.get("info") is not None
        ):
            dump_all_feature_maps(
                model=self.model,
                video=video,
                video_info=batch.get("info", None),
                attn_map=None,
                save_root=f"{self.save_root}/test_all_feature_maps/{fold}/batch_{batch_idx}",
                include_types=(torch.nn.Conv3d, torch.nn.Linear),
                include_name_contains=["conv_c"],
                resize_to=(256, 256),  # 指定输出大小
                resize_mode="bilinear",  # 放大更平滑
            )

        return video_preds_softmax, video_preds

    def on_test_epoch_end(self) -> None:
        """hook function for test epoch end"""

        # save the metrics to file
        save_helper(
            all_pred=self.test_pred_list,
            all_label=self.test_label_list,
            fold=self.logger.root_dir.split("/")[-1],
            save_path=self.save_root,
            num_class=self.num_classes,
        )

        logger.info("test epoch end")

    def configure_optimizers(self):
        """
        configure the optimizer and lr scheduler

        Returns:
            optimizer: the used optimizer.
            lr_scheduler: the selected lr scheduler.
        """

        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": torch.optim.lr_scheduler.CosineAnnealingLR(
                    optimizer,
                    T_max=self.trainer.estimated_stepping_batches,
                    # verbose=True,
                ),
                "monitor": "train/loss",
            },
        }


class TransformerTrainer(Res3DCNNTrainer):
    """Single-view trainer alias for transformer backbone routing."""


class MambaTrainer(Res3DCNNTrainer):
    """Single-view trainer alias for mamba backbone routing."""


class STGCNTrainer(Res3DCNNTrainer):
    """Single-view trainer alias for ST-GCN backbone routing."""
